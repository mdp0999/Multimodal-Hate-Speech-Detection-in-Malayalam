{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48798cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'C': 0, 'G': 1, 'N': 2, 'P': 3, 'R': 4}\n",
      "['File Name', 'Class Label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:08<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.3087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 14.07it/s]\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/scikit_learn-1.5.1-py3.11-linux-x86_64.egg/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/scikit_learn-1.5.1-py3.11-linux-x86_64.egg/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/scikit_learn-1.5.1-py3.11-linux-x86_64.egg/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/scikit_learn-1.5.1-py3.11-linux-x86_64.egg/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/scikit_learn-1.5.1-py3.11-linux-x86_64.egg/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/scikit_learn-1.5.1-py3.11-linux-x86_64.egg/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.0899, Accuracy: 0.5600\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.33      0.90      0.49        10\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.56        50\n",
      "   macro avg       0.40      0.56      0.45        50\n",
      "weighted avg       0.40      0.56      0.45        50\n",
      "\n",
      "New best Macro F1-Score: 0.4505. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:00<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Loss: 1.8852, Accuracy: 0.6000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69        10\n",
      "           1       0.56      0.50      0.53        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.56      0.50      0.53        10\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.49      0.60      0.53        50\n",
      "weighted avg       0.49      0.60      0.53        50\n",
      "\n",
      "New best Macro F1-Score: 0.5303. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:06<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation Loss: 1.4852, Accuracy: 0.6000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        10\n",
      "           1       1.00      0.50      0.67        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "           3       0.50      0.10      0.17        10\n",
      "           4       0.40      0.40      0.40        10\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.65      0.60      0.56        50\n",
      "weighted avg       0.65      0.60      0.56        50\n",
      "\n",
      "New best Macro F1-Score: 0.5621. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:04<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation Loss: 1.5457, Accuracy: 0.7800\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        10\n",
      "           1       0.78      0.70      0.74        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.75      0.60      0.67        10\n",
      "           4       0.67      0.60      0.63        10\n",
      "\n",
      "    accuracy                           0.78        50\n",
      "   macro avg       0.77      0.78      0.77        50\n",
      "weighted avg       0.77      0.78      0.77        50\n",
      "\n",
      "New best Macro F1-Score: 0.7707. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Validation Loss: 1.5692, Accuracy: 0.8000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        10\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.71      1.00      0.83        10\n",
      "           3       0.83      0.50      0.62        10\n",
      "           4       0.73      0.80      0.76        10\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.82      0.80      0.79        50\n",
      "weighted avg       0.82      0.80      0.79        50\n",
      "\n",
      "New best Macro F1-Score: 0.7906. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training Loss: 0.1708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Validation Loss: 1.7805, Accuracy: 0.8000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        10\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.71      1.00      0.83        10\n",
      "           3       0.86      0.60      0.71        10\n",
      "           4       0.70      0.70      0.70        10\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.82      0.80      0.79        50\n",
      "weighted avg       0.82      0.80      0.79        50\n",
      "\n",
      "New best Macro F1-Score: 0.7944. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:01<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training Loss: 0.1453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Validation Loss: 1.5560, Accuracy: 0.8400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        10\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.88      0.70      0.78        10\n",
      "           4       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.84      0.84        50\n",
      "weighted avg       0.86      0.84      0.84        50\n",
      "\n",
      "New best Macro F1-Score: 0.8360. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:06<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training Loss: 0.1377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Validation Loss: 1.5455, Accuracy: 0.8400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        10\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.88      0.70      0.78        10\n",
      "           4       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.84      0.84        50\n",
      "weighted avg       0.86      0.84      0.84        50\n",
      "\n",
      "Macro Average F1-Score: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:01<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training Loss: 0.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validation]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Validation Loss: 1.5748, Accuracy: 0.8400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        10\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.88      0.70      0.78        10\n",
      "           4       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.84      0.84        50\n",
      "weighted avg       0.86      0.84      0.84        50\n",
      "\n",
      "Macro Average F1-Score: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 883/883 [04:02<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 0.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validation]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Validation Loss: 1.5910, Accuracy: 0.8400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        10\n",
      "           1       1.00      0.70      0.82        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       0.88      0.70      0.78        10\n",
      "           4       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.86      0.84      0.84        50\n",
      "weighted avg       0.86      0.84      0.84        50\n",
      "\n",
      "Macro Average F1-Score: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, Wav2Vec2Model, Wav2Vec2Processor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "\n",
    "# Paths to audio and text data\n",
    "AUDIO_FOLDER = \"./malayalam/audio/\"\n",
    "TEXT_FILE = \"./malayalam/text/ML-AT-train.csv\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LSTM_LAYERS = 2\n",
    "DROPOUT_PROB = 0.3\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torchaudio.transforms as transforms\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, audio_paths, transcripts, labels, audio_processor, text_tokenizer, max_length=48):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.transcripts = transcripts\n",
    "        self.labels = labels\n",
    "        self.audio_processor = audio_processor\n",
    "        self.text_tokenizer =  text_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.resampler = transforms.Resample(orig_freq=44100, new_freq=16000)  # Adjust orig_freq as needed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Resample audio if necessary\n",
    "        if sr != 16000:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        # Preprocess audio\n",
    "        audio_features = self.audio_processor(\n",
    "            waveform.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).input_values[0]\n",
    "\n",
    "        # Tokenize transcript\n",
    "        transcript = self.transcripts[idx]\n",
    "        text_encoding = self.text_tokenizer(\n",
    "            transcript,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Extract input_ids and attention_mask\n",
    "        input_ids = text_encoding['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = text_encoding['attention_mask'].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            \"audio_features\": audio_features,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class BilstmXLMRobertaWav2VecClassifier(nn.Module):\n",
    "    def __init__(self, xlm_model_name: str, wav2vec_model_name: str, num_labels: int,\n",
    "                 lstm_hidden_size: int, lstm_layers: int, dropout_prob: float):\n",
    "        super(BilstmXLMRobertaWav2VecClassifier, self).__init__()\n",
    "\n",
    "        # Text embeddings using XLM-RoBERTa\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            xlm_model_name, cache_dir=\"./telugu_lm\"\n",
    "        )\n",
    "\n",
    "        # Speech embeddings using Wav2Vec 2.0\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\n",
    "            wav2vec_model_name\n",
    "        )\n",
    "\n",
    "        # BiLSTM for combined embeddings\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=768,  # Combined hidden sizes\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(lstm_hidden_size * 2, 1)\n",
    "\n",
    "        # Dropout and layer normalization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(lstm_hidden_size * 2)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                audio_features: torch.Tensor, labels: torch.Tensor = None):\n",
    "        # Text embeddings from XLM-RoBERTa\n",
    "        text_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state  # (batch_size, seq_len, roberta_hidden_size)\n",
    "\n",
    "        # Speech embeddings from Wav2Vec\n",
    "        speech_outputs = self.wav2vec(audio_features)\n",
    "        speech_embeddings = speech_outputs.last_hidden_state  # (batch_size, seq_len, wav2vec_hidden_size)\n",
    "        \n",
    "        # Concatenate text and speech embeddings\n",
    "        combined_embeddings = torch.cat([text_embeddings, speech_embeddings], dim=1)  # (batch_size, total_seq_len, combined_hidden_size)\n",
    "\n",
    "        # BiLSTM for combined embeddings\n",
    "        lstm_output, _ = self.bilstm(combined_embeddings)  # (batch_size, total_seq_len, lstm_hidden_size * 2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(torch.tanh(self.attention(lstm_output)), dim=1)  # (batch_size, total_seq_len, 1)\n",
    "        attention_output = torch.sum(lstm_output * attention_weights, dim=1)  # (batch_size, lstm_hidden_size * 2)\n",
    "\n",
    "        # Normalize and apply dropout\n",
    "        attention_output = self.layer_norm(attention_output)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "\n",
    "        # Classification logits\n",
    "        logits = self.classifier(attention_output)  # (batch_size, num_labels)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_data(audio_folder, text_file):\n",
    "    audio_paths, transcripts, labels = [], [], []\n",
    "    df = pd.read_csv(text_file)\n",
    "    for _, row in df.iterrows():\n",
    "        file_name = row[\"File Name\"]\n",
    "        transcript = row[\"Transcript\"]\n",
    "        label = row[\"Class Label Short\"]\n",
    "\n",
    "        audio_path = os.path.join(audio_folder, file_name + \".wav\")\n",
    "        if os.path.exists(audio_path):\n",
    "            audio_paths.append(audio_path)\n",
    "            transcripts.append(transcript)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    return audio_paths, transcripts, labels\n",
    "\n",
    "\n",
    "# Model names\n",
    "xlm_model_name = \"l3cube-pune/malayalam-topic-all-doc\"\n",
    "wav2vec_model_name = \"./ml_w2v\"\n",
    "\n",
    "# Load processors\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(wav2vec_model_name)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(xlm_model_name, cache_dir=\"./malayalam_lm\")\n",
    "\n",
    "# Load data\n",
    "train_audio, train_transcripts, train_labels = load_data(AUDIO_FOLDER, TEXT_FILE)\n",
    "unique_labels = list(set(train_labels))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Labels:\", label_mapping)\n",
    "NUM_CLASSES = len(unique_labels)\n",
    "\n",
    "train_labels = [label_mapping[label] for label in train_labels]\n",
    "\n",
    "test_text_file = \"./test/malayalam/text/ML-AT-test.csv\"\n",
    "test_audio_folder = \"./test/malayalam/audio/\"\n",
    "test_label_paths = \"ML-AT-test.xlsx - Sheet1.csv\"\n",
    "df_test1 = pd.read_csv(test_label_paths)\n",
    "print(list(df_test1))\n",
    "test_pathsMap = {path:label for path, label in zip(df_test1[\"File Name\"],df_test1[\"Class Label\"])}\n",
    "\n",
    "val_audio, val_transcripts, val_labels = [], [], []\n",
    "df_test = pd.read_csv(test_text_file)\n",
    "for name, trans in zip(df_test[\"File Name\"],df_test[\"Transcript\"]):\n",
    "    label = test_pathsMap[name]\n",
    "    val_labels.append(label_mapping[label])\n",
    "    audio_path = os.path.join(test_audio_folder, name + \".wav\")\n",
    "    val_audio.append(audio_path)\n",
    "    val_transcripts.append(trans)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio_features = [item['audio_features'] for item in batch]\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "\n",
    "    # Pad sequences\n",
    "    audio_features_padded = pad_sequence(audio_features, batch_first=True)\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'audio_features': audio_features_padded,\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MultiModalDataset(train_audio, train_transcripts, train_labels, audio_processor, text_tokenizer)\n",
    "val_dataset = MultiModalDataset(val_audio, val_transcripts, val_labels, audio_processor, text_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the model\n",
    "model = BilstmXLMRobertaWav2VecClassifier(\n",
    "    xlm_model_name=xlm_model_name,\n",
    "    wav2vec_model_name=wav2vec_model_name,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    lstm_hidden_size=HIDDEN_SIZE,\n",
    "    lstm_layers=NUM_LSTM_LAYERS,\n",
    "    dropout_prob=DROPOUT_PROB\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Initialize variables for saving the best model\n",
    "best_macro_f1 = 0.0\n",
    "best_model_path = \"best_model_ml1.pth\"\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Training]\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        audio_features = batch[\"audio_features\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        loss, _ = model(input_ids=input_ids, attention_mask=attention_mask, audio_features=audio_features, labels=labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Validation]\"):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            audio_features = batch[\"audio_features\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            loss, logits = model(input_ids=input_ids, attention_mask=attention_mask, audio_features=audio_features, labels=labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    print(report)\n",
    "    \n",
    "    # Generate the classification report as a dictionary\n",
    "    report1 = classification_report(all_labels,all_preds,output_dict=True)\n",
    "    # Extract macro average F1-score\n",
    "    macro_f1 = report1['macro avg']['f1-score']\n",
    "\n",
    "    # Save the model if it has the best macro F1-score\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best Macro F1-Score: {best_macro_f1:.4f}. Saving model...\")\n",
    "    else:\n",
    "        print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b5e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d277bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rohit/anaconda3/envs/xlstm/lib/python3.11/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at ./ml_w2v and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_15962/1158453623.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
      "Inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to 'byteSizedLLM_malayalam_run1.tsv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SingleSampleMultiModalDataset(Dataset):\n",
    "    def __init__(self, audio_path, transcript, audio_processor, text_tokenizer, max_length=48):\n",
    "        self.audio_path = audio_path\n",
    "        self.transcript = transcript\n",
    "        self.audio_processor = audio_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.resampler = transforms.Resample(orig_freq=44100, new_freq=16000)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx=None):\n",
    "        waveform, sr = torchaudio.load(self.audio_path)\n",
    "        if sr != 16000:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        audio_features = self.audio_processor(\n",
    "            waveform.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).input_values[0]\n",
    "\n",
    "        text_encoding = self.text_tokenizer(\n",
    "            self.transcript,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = text_encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = text_encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"audio_features\": audio_features,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "model = BilstmXLMRobertaWav2VecClassifier(\n",
    "    xlm_model_name=xlm_model_name,\n",
    "    wav2vec_model_name=wav2vec_model_name,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    lstm_hidden_size=HIDDEN_SIZE,\n",
    "    lstm_layers=NUM_LSTM_LAYERS,\n",
    "    dropout_prob=DROPOUT_PROB\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# Reverse label mapping\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Load test data\n",
    "test_text_file = \"./test/malayalam/text/ML-AT-test.csv\"\n",
    "test_audio_folder = \"./test/malayalam/audio/\"\n",
    "df_test = pd.read_csv(test_text_file)\n",
    "\n",
    "# Inference loop\n",
    "predictions = []\n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Inference\"):\n",
    "    file_name = row[\"File Name\"]\n",
    "    transcript = row[\"Transcript\"]\n",
    "    audio_path = f\"{test_audio_folder}/{file_name}.wav\"\n",
    "\n",
    "    # Create dataset instance for the sample\n",
    "    dataset = SingleSampleMultiModalDataset(audio_path, transcript, audio_processor, text_tokenizer)\n",
    "    processed_data = dataset[0]  # Access the single sample\n",
    "\n",
    "    # Move tensors to device\n",
    "    input_ids = processed_data[\"input_ids\"].unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
    "    attention_mask = processed_data[\"attention_mask\"].unsqueeze(0).to(DEVICE)\n",
    "    audio_features = processed_data[\"audio_features\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Model prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, audio_features=audio_features)\n",
    "        pred_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Append results\n",
    "    predictions.append([file_name, reverse_label_mapping[pred_label]])\n",
    "\n",
    "# Save predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"File Name\", \"Class Label Short\"])\n",
    "\n",
    "# Save predictions to a TSV file\n",
    "output_csv = \"byteSizedLLM_malayalam_run1.tsv\"\n",
    "predictions_df.to_csv(output_csv, sep=\"\\t\", index=False)\n",
    "print(f\"Inference complete. Predictions saved to '{output_csv}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751c31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02be649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to 'byteSizedLLM_malayalam_run2.tsv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SingleSampleMultiModalDataset(Dataset):\n",
    "    def __init__(self, audio_path, transcript, audio_processor, text_tokenizer, max_length=48):\n",
    "        self.audio_path = audio_path\n",
    "        self.transcript = transcript\n",
    "        self.audio_processor = audio_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.resampler = transforms.Resample(orig_freq=44100, new_freq=16000)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx=None):\n",
    "        waveform, sr = torchaudio.load(self.audio_path)\n",
    "        if sr != 16000:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        audio_features = self.audio_processor(\n",
    "            waveform.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).input_values[0]\n",
    "\n",
    "        text_encoding = self.text_tokenizer(\n",
    "            self.transcript,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = text_encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = text_encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"audio_features\": audio_features,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Reverse label mapping\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Load test data\n",
    "test_text_file = \"./test/malayalam/text/ML-AT-test.csv\"\n",
    "test_audio_folder = \"./test/malayalam/audio/\"\n",
    "df_test = pd.read_csv(test_text_file)\n",
    "\n",
    "# Inference loop\n",
    "predictions = []\n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Inference\"):\n",
    "    file_name = row[\"File Name\"]\n",
    "    transcript = row[\"Transcript\"]\n",
    "    audio_path = f\"{test_audio_folder}/{file_name}.wav\"\n",
    "\n",
    "    # Create dataset instance for the sample\n",
    "    dataset = SingleSampleMultiModalDataset(audio_path, transcript, audio_processor, text_tokenizer)\n",
    "    processed_data = dataset[0]  # Access the single sample\n",
    "\n",
    "    # Move tensors to device\n",
    "    input_ids = processed_data[\"input_ids\"].unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
    "    attention_mask = processed_data[\"attention_mask\"].unsqueeze(0).to(DEVICE)\n",
    "    audio_features = processed_data[\"audio_features\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Model prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, audio_features=audio_features)\n",
    "        pred_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Append results\n",
    "    predictions.append([file_name, reverse_label_mapping[pred_label]])\n",
    "\n",
    "# Save predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"File Name\", \"Class Label Short\"])\n",
    "\n",
    "# Save predictions to a TSV file\n",
    "output_csv = \"byteSizedLLM_malayalam_run2.tsv\"\n",
    "predictions_df.to_csv(output_csv, sep=\"\\t\", index=False)\n",
    "print(f\"Inference complete. Predictions saved to '{output_csv}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc1a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xlstm] *",
   "language": "python",
   "name": "conda-env-xlstm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
